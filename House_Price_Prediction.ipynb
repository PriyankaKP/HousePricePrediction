{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction - Surprise Housing\n",
    "## Ridge and Lasso Regression Analysis\n",
    "\n",
    "### Business Objective\n",
    "Build a regression model using regularization to predict house prices in the Australian market and identify significant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Understand the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Display basic information\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and non-null counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical features\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "print(\"Target Variable (SalePrice) Statistics:\")\n",
    "print(df['SalePrice'].describe())\n",
    "\n",
    "# Visualize target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['SalePrice'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Sale Price')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Sale Price')\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(df['SalePrice'], dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot of Sale Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSkewness: {df['SalePrice'].skew():.2f}\")\n",
    "print(f\"Kurtosis: {df['SalePrice'].kurtosis():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Missing_Percent': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "print(f\"Total features with missing values: {len(missing_df)}\")\n",
    "print(\"\\nTop features with missing values:\")\n",
    "print(missing_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "if len(missing_df) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_df.head(20)['Missing_Percent'].plot(kind='barh')\n",
    "    plt.xlabel('Missing Percentage')\n",
    "    plt.title('Top 20 Features with Missing Values')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# If duplicates exist, remove them\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {duplicates} duplicate rows\")\n",
    "    print(f\"New dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical features\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove 'Id' from numerical features if present\n",
    "if 'Id' in numerical_features:\n",
    "    numerical_features.remove('Id')\n",
    "\n",
    "# Remove 'SalePrice' from numerical features for correlation analysis\n",
    "numerical_features_without_target = [col for col in numerical_features if col != 'SalePrice']\n",
    "\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with target variable\n",
    "correlations = df[numerical_features].corr()['SalePrice'].sort_values(ascending=False)\n",
    "print(\"Top 15 features correlated with SalePrice:\")\n",
    "print(correlations.head(15))\n",
    "\n",
    "print(\"\\nBottom 10 features correlated with SalePrice:\")\n",
    "print(correlations.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap for top features\n",
    "top_features = correlations.head(11).index.tolist()  # Top 10 + SalePrice\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df[top_features].corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap of Top Features with SalePrice')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for top correlated features\n",
    "top_corr_features = correlations.head(6).index.tolist()[1:]  # Exclude SalePrice itself\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_corr_features[:6]):\n",
    "    axes[idx].scatter(df[feature], df['SalePrice'], alpha=0.5)\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('SalePrice')\n",
    "    axes[idx].set_title(f'{feature} vs SalePrice (r={correlations[feature]:.2f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features - check unique values\n",
    "print(\"Categorical Features - Unique Value Counts:\")\n",
    "for col in categorical_features[:10]:  # Show first 10\n",
    "    print(f\"\\n{col}: {df[col].nunique()} unique values\")\n",
    "    print(df[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for important categorical features\n",
    "important_cat_features = ['OverallQual', 'Neighborhood', 'ExterQual', 'KitchenQual']\n",
    "\n",
    "for feature in important_cat_features:\n",
    "    if feature in df.columns:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        if df[feature].dtype == 'object':\n",
    "            order = df.groupby(feature)['SalePrice'].median().sort_values(ascending=False).index\n",
    "            sns.boxplot(x=feature, y='SalePrice', data=df, order=order)\n",
    "        else:\n",
    "            sns.boxplot(x=feature, y='SalePrice', data=df)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f'SalePrice Distribution by {feature}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Drop 'Id' column as it's not useful for prediction\n",
    "if 'Id' in df_processed.columns:\n",
    "    df_processed = df_processed.drop('Id', axis=1)\n",
    "\n",
    "print(f\"Dataset shape after dropping Id: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For features where NA means 'None', fill with 'None'\n",
    "none_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', \n",
    "                 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', \n",
    "                 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\n",
    "\n",
    "for feature in none_features:\n",
    "    if feature in df_processed.columns:\n",
    "        df_processed[feature] = df_processed[feature].fillna('None')\n",
    "\n",
    "print(\"Filled NA values with 'None' for categorical features where NA means absence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numerical features, fill with 0 where NA means absence\n",
    "zero_features = ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', \n",
    "                 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\n",
    "\n",
    "for feature in zero_features:\n",
    "    if feature in df_processed.columns:\n",
    "        df_processed[feature] = df_processed[feature].fillna(0)\n",
    "\n",
    "print(\"Filled NA values with 0 for numerical features where NA means absence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LotFrontage, fill with median by neighborhood\n",
    "if 'LotFrontage' in df_processed.columns:\n",
    "    df_processed['LotFrontage'] = df_processed.groupby('Neighborhood')['LotFrontage'].transform(\n",
    "        lambda x: x.fillna(x.median()))\n",
    "    print(\"Filled LotFrontage with neighborhood median\")\n",
    "\n",
    "# For Electrical, fill with mode\n",
    "if 'Electrical' in df_processed.columns:\n",
    "    df_processed['Electrical'] = df_processed['Electrical'].fillna(df_processed['Electrical'].mode()[0])\n",
    "    print(\"Filled Electrical with mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with too many missing values (>40%)\n",
    "threshold = 0.4\n",
    "missing_pct = df_processed.isnull().sum() / len(df_processed)\n",
    "cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping columns with >{threshold*100}% missing values: {cols_to_drop}\")\n",
    "    df_processed = df_processed.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Fill remaining missing values\n",
    "for col in df_processed.columns:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        if df_processed[col].dtype == 'object':\n",
    "            df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0])\n",
    "        else:\n",
    "            df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "\n",
    "print(f\"\\nRemaining missing values: {df_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers in target variable (SalePrice)\n",
    "# Remove extreme outliers using IQR method\n",
    "Q1 = df_processed['SalePrice'].quantile(0.25)\n",
    "Q3 = df_processed['SalePrice'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 3 * IQR  # Using 3*IQR for more conservative outlier removal\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "outliers = df_processed[(df_processed['SalePrice'] < lower_bound) | (df_processed['SalePrice'] > upper_bound)]\n",
    "print(f\"Number of outliers in SalePrice: {len(outliers)}\")\n",
    "\n",
    "# Remove outliers\n",
    "df_processed = df_processed[(df_processed['SalePrice'] >= lower_bound) & (df_processed['SalePrice'] <= upper_bound)]\n",
    "print(f\"Dataset shape after outlier removal: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features based on domain knowledge\n",
    "\n",
    "# Total square footage\n",
    "if all(col in df_processed.columns for col in ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']):\n",
    "    df_processed['TotalSF'] = df_processed['TotalBsmtSF'] + df_processed['1stFlrSF'] + df_processed['2ndFlrSF']\n",
    "    print(\"Created TotalSF feature\")\n",
    "\n",
    "# Total bathrooms\n",
    "if all(col in df_processed.columns for col in ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath']):\n",
    "    df_processed['TotalBathrooms'] = (df_processed['BsmtFullBath'] + df_processed['BsmtHalfBath'] * 0.5 + \n",
    "                                       df_processed['FullBath'] + df_processed['HalfBath'] * 0.5)\n",
    "    print(\"Created TotalBathrooms feature\")\n",
    "\n",
    "# Total porch area\n",
    "porch_cols = ['WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n",
    "if all(col in df_processed.columns for col in porch_cols):\n",
    "    df_processed['TotalPorchSF'] = df_processed[porch_cols].sum(axis=1)\n",
    "    print(\"Created TotalPorchSF feature\")\n",
    "\n",
    "# House age\n",
    "if 'YearBuilt' in df_processed.columns and 'YrSold' in df_processed.columns:\n",
    "    df_processed['HouseAge'] = df_processed['YrSold'] - df_processed['YearBuilt']\n",
    "    print(\"Created HouseAge feature\")\n",
    "\n",
    "# Years since remodel\n",
    "if 'YearRemodAdd' in df_processed.columns and 'YrSold' in df_processed.columns:\n",
    "    df_processed['YearsSinceRemod'] = df_processed['YrSold'] - df_processed['YearRemodAdd']\n",
    "    print(\"Created YearsSinceRemod feature\")\n",
    "\n",
    "# Is house remodeled?\n",
    "if 'YearBuilt' in df_processed.columns and 'YearRemodAdd' in df_processed.columns:\n",
    "    df_processed['IsRemodeled'] = (df_processed['YearRemodAdd'] != df_processed['YearBuilt']).astype(int)\n",
    "    print(\"Created IsRemodeled feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical columns\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Number of categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# Create dummy variables\n",
    "df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"\\nDataset shape after creating dummy variables: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_processed.drop('SalePrice', axis=1)\n",
    "y = df_processed['SalePrice']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (70:30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling - StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier interpretation\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Building - Linear Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
    "y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "r2_train_lr = r2_score(y_train, y_train_pred_lr)\n",
    "r2_test_lr = r2_score(y_test, y_test_pred_lr)\n",
    "rmse_train_lr = np.sqrt(mean_squared_error(y_train, y_train_pred_lr))\n",
    "rmse_test_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LINEAR REGRESSION (Baseline Model)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training R² Score: {r2_train_lr:.4f}\")\n",
    "print(f\"Testing R² Score: {r2_test_lr:.4f}\")\n",
    "print(f\"Training RMSE: ${rmse_train_lr:,.2f}\")\n",
    "print(f\"Testing RMSE: ${rmse_test_lr:,.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Building - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with cross-validation to find optimal alpha\n",
    "# Test a range of alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5, scoring='r2')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "optimal_alpha_ridge = ridge_cv.alpha_\n",
    "print(f\"Optimal Alpha for Ridge Regression: {optimal_alpha_ridge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Ridge model with optimal alpha\n",
    "ridge_model = Ridge(alpha=optimal_alpha_ridge)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ridge = ridge_model.predict(X_train_scaled)\n",
    "y_test_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "r2_train_ridge = r2_score(y_train, y_train_pred_ridge)\n",
    "r2_test_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "rmse_train_ridge = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))\n",
    "rmse_test_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
    "mae_test_ridge = mean_absolute_error(y_test, y_test_pred_ridge)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"RIDGE REGRESSION (Alpha = {optimal_alpha_ridge})\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training R² Score: {r2_train_ridge:.4f}\")\n",
    "print(f\"Testing R² Score: {r2_test_ridge:.4f}\")\n",
    "print(f\"Training RMSE: ${rmse_train_ridge:,.2f}\")\n",
    "print(f\"Testing RMSE: ${rmse_test_ridge:,.2f}\")\n",
    "print(f\"Testing MAE: ${mae_test_ridge:,.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation score for Ridge\n",
    "ridge_cv_scores = cross_val_score(ridge_model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "print(f\"\\nRidge Cross-Validation R² Scores: {ridge_cv_scores}\")\n",
    "print(f\"Mean CV R² Score: {ridge_cv_scores.mean():.4f} (+/- {ridge_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for Ridge\n",
    "ridge_coef = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': ridge_model.coef_\n",
    "})\n",
    "ridge_coef['Abs_Coefficient'] = np.abs(ridge_coef['Coefficient'])\n",
    "ridge_coef = ridge_coef.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features (Ridge):\")\n",
    "print(ridge_coef.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features for Ridge\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20_ridge = ridge_coef.head(20)\n",
    "plt.barh(range(len(top_20_ridge)), top_20_ridge['Coefficient'])\n",
    "plt.yticks(range(len(top_20_ridge)), top_20_ridge['Feature'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Top 20 Feature Coefficients - Ridge Regression')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Building - Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression with cross-validation to find optimal alpha\n",
    "alphas_lasso = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alphas_lasso, cv=5, random_state=42, max_iter=10000)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "optimal_alpha_lasso = lasso_cv.alpha_\n",
    "print(f\"Optimal Alpha for Lasso Regression: {optimal_alpha_lasso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lasso model with optimal alpha\n",
    "lasso_model = Lasso(alpha=optimal_alpha_lasso, random_state=42, max_iter=10000)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lasso = lasso_model.predict(X_train_scaled)\n",
    "y_test_pred_lasso = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "r2_train_lasso = r2_score(y_train, y_train_pred_lasso)\n",
    "r2_test_lasso = r2_score(y_test, y_test_pred_lasso)\n",
    "rmse_train_lasso = np.sqrt(mean_squared_error(y_train, y_train_pred_lasso))\n",
    "rmse_test_lasso = np.sqrt(mean_squared_error(y_test, y_test_pred_lasso))\n",
    "mae_test_lasso = mean_absolute_error(y_test, y_test_pred_lasso)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"LASSO REGRESSION (Alpha = {optimal_alpha_lasso})\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training R² Score: {r2_train_lasso:.4f}\")\n",
    "print(f\"Testing R² Score: {r2_test_lasso:.4f}\")\n",
    "print(f\"Training RMSE: ${rmse_train_lasso:,.2f}\")\n",
    "print(f\"Testing RMSE: ${rmse_test_lasso:,.2f}\")\n",
    "print(f\"Testing MAE: ${mae_test_lasso:,.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation score for Lasso\n",
    "lasso_cv_scores = cross_val_score(lasso_model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "print(f\"\\nLasso Cross-Validation R² Scores: {lasso_cv_scores}\")\n",
    "print(f\"Mean CV R² Score: {lasso_cv_scores.mean():.4f} (+/- {lasso_cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for Lasso\n",
    "lasso_coef = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lasso_model.coef_\n",
    "})\n",
    "lasso_coef['Abs_Coefficient'] = np.abs(lasso_coef['Coefficient'])\n",
    "lasso_coef = lasso_coef.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Count non-zero coefficients\n",
    "non_zero_coef = (lasso_coef['Coefficient'] != 0).sum()\n",
    "print(f\"\\nNumber of features selected by Lasso: {non_zero_coef} out of {len(lasso_coef)}\")\n",
    "print(f\"Number of features eliminated: {len(lasso_coef) - non_zero_coef}\")\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features (Lasso):\")\n",
    "print(lasso_coef.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features for Lasso\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20_lasso = lasso_coef[lasso_coef['Coefficient'] != 0].head(20)\n",
    "plt.barh(range(len(top_20_lasso)), top_20_lasso['Coefficient'])\n",
    "plt.yticks(range(len(top_20_lasso)), top_20_lasso['Feature'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Top 20 Feature Coefficients - Lasso Regression')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Ridge', 'Lasso'],\n",
    "    'Train R²': [r2_train_lr, r2_train_ridge, r2_train_lasso],\n",
    "    'Test R²': [r2_test_lr, r2_test_ridge, r2_test_lasso],\n",
    "    'Train RMSE': [rmse_train_lr, rmse_train_ridge, rmse_train_lasso],\n",
    "    'Test RMSE': [rmse_test_lr, rmse_test_ridge, rmse_test_lasso],\n",
    "    'Test MAE': [mean_absolute_error(y_test, y_test_pred_lr), mae_test_ridge, mae_test_lasso]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R² comparison\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, comparison_df['Train R²'], width, label='Train R²')\n",
    "axes[0].bar(x + width/2, comparison_df['Test R²'], width, label='Test R²')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('R² Score Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comparison_df['Model'])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].bar(x - width/2, comparison_df['Train RMSE'], width, label='Train RMSE')\n",
    "axes[1].bar(x + width/2, comparison_df['Test RMSE'], width, label='Test RMSE')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('RMSE Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comparison_df['Model'])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Linear Regression residuals\n",
    "residuals_lr = y_test - y_test_pred_lr\n",
    "axes[0].scatter(y_test_pred_lr, residuals_lr, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Linear Regression - Residual Plot')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ridge residuals\n",
    "residuals_ridge = y_test - y_test_pred_ridge\n",
    "axes[1].scatter(y_test_pred_ridge, residuals_ridge, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Ridge Regression - Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso residuals\n",
    "residuals_lasso = y_test - y_test_pred_lasso\n",
    "axes[2].scatter(y_test_pred_lasso, residuals_lasso, alpha=0.5)\n",
    "axes[2].axhline(y=0, color='r', linestyle='--')\n",
    "axes[2].set_xlabel('Predicted Values')\n",
    "axes[2].set_ylabel('Residuals')\n",
    "axes[2].set_title('Lasso Regression - Residual Plot')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Linear Regression\n",
    "axes[0].scatter(y_test, y_test_pred_lr, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Price')\n",
    "axes[0].set_ylabel('Predicted Price')\n",
    "axes[0].set_title(f'Linear Regression (R²={r2_test_lr:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ridge\n",
    "axes[1].scatter(y_test, y_test_pred_ridge, alpha=0.5)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Price')\n",
    "axes[1].set_ylabel('Predicted Price')\n",
    "axes[1].set_title(f'Ridge Regression (R²={r2_test_ridge:.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso\n",
    "axes[2].scatter(y_test, y_test_pred_lasso, alpha=0.5)\n",
    "axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[2].set_xlabel('Actual Price')\n",
    "axes[2].set_ylabel('Predicted Price')\n",
    "axes[2].set_title(f'Lasso Regression (R²={r2_test_lasso:.4f})')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Alpha Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Ridge with double alpha\n",
    "ridge_2x = Ridge(alpha=optimal_alpha_ridge * 2)\n",
    "ridge_2x.fit(X_train_scaled, y_train)\n",
    "y_test_pred_ridge_2x = ridge_2x.predict(X_test_scaled)\n",
    "r2_ridge_2x = r2_score(y_test, y_test_pred_ridge_2x)\n",
    "\n",
    "print(f\"Ridge with 2x Alpha ({optimal_alpha_ridge * 2}):\")\n",
    "print(f\"R² Score: {r2_ridge_2x:.4f}\")\n",
    "print(f\"Change in R²: {r2_ridge_2x - r2_test_ridge:.4f}\")\n",
    "\n",
    "# Get top features\n",
    "ridge_2x_coef = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': ridge_2x.coef_\n",
    "})\n",
    "ridge_2x_coef['Abs_Coefficient'] = np.abs(ridge_2x_coef['Coefficient'])\n",
    "ridge_2x_coef = ridge_2x_coef.sort_values('Abs_Coefficient', ascending=False)\n",
    "print(\"\\nTop 10 Features with 2x Alpha (Ridge):\")\n",
    "print(ridge_2x_coef.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Lasso with double alpha\n",
    "lasso_2x = Lasso(alpha=optimal_alpha_lasso * 2, random_state=42, max_iter=10000)\n",
    "lasso_2x.fit(X_train_scaled, y_train)\n",
    "y_test_pred_lasso_2x = lasso_2x.predict(X_test_scaled)\n",
    "r2_lasso_2x = r2_score(y_test, y_test_pred_lasso_2x)\n",
    "\n",
    "print(f\"\\nLasso with 2x Alpha ({optimal_alpha_lasso * 2}):\")\n",
    "print(f\"R² Score: {r2_lasso_2x:.4f}\")\n",
    "print(f\"Change in R²: {r2_lasso_2x - r2_test_lasso:.4f}\")\n",
    "\n",
    "# Get top features\n",
    "lasso_2x_coef = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lasso_2x.coef_\n",
    "})\n",
    "lasso_2x_coef['Abs_Coefficient'] = np.abs(lasso_2x_coef['Coefficient'])\n",
    "lasso_2x_coef = lasso_2x_coef.sort_values('Abs_Coefficient', ascending=False)\n",
    "non_zero_2x = (lasso_2x_coef['Coefficient'] != 0).sum()\n",
    "print(f\"\\nNumber of features selected: {non_zero_2x}\")\n",
    "print(\"\\nTop 10 Features with 2x Alpha (Lasso):\")\n",
    "print(lasso_2x_coef[lasso_2x_coef['Coefficient'] != 0].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS AND BUSINESS RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. OPTIMAL ALPHA VALUES:\")\n",
    "print(f\"   - Ridge Regression: α = {optimal_alpha_ridge}\")\n",
    "print(f\"   - Lasso Regression: α = {optimal_alpha_lasso}\")\n",
    "\n",
    "print(\"\\n2. MODEL PERFORMANCE:\")\n",
    "print(f\"   - Lasso R² Score: {r2_test_lasso:.4f}\")\n",
    "print(f\"   - Ridge R² Score: {r2_test_ridge:.4f}\")\n",
    "print(f\"   - Lasso selected {non_zero_coef} features out of {len(lasso_coef)}\")\n",
    "\n",
    "print(\"\\n3. TOP 5 MOST IMPORTANT PREDICTORS (Lasso):\")\n",
    "top_5_features = lasso_coef[lasso_coef['Coefficient'] != 0].head(5)\n",
    "for idx, row in top_5_features.iterrows():\n",
    "    print(f\"   {idx+1}. {row['Feature']}: {row['Coefficient']:.2f}\")\n",
    "\n",
    "print(\"\\n4. BUSINESS INSIGHTS:\")\n",
    "print(\"   - Overall quality is the strongest price predictor\")\n",
    "print(\"   - Living area size significantly impacts price\")\n",
    "print(\"   - Location (neighborhood) plays a crucial role\")\n",
    "print(\"   - Recent construction/renovation adds substantial value\")\n",
    "print(\"   - Basement and garage features are important\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDED MODEL:\")\n",
    "if r2_test_lasso >= r2_test_ridge:\n",
    "    print(\"   - LASSO Regression (better feature selection and interpretability)\")\n",
    "else:\n",
    "    print(\"   - RIDGE Regression (slightly better performance)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Results for Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save top features from Lasso model\n",
    "lasso_features_selected = lasso_coef[lasso_coef['Coefficient'] != 0].copy()\n",
    "lasso_features_selected.to_csv('lasso_selected_features.csv', index=False)\n",
    "print(f\"Saved {len(lasso_features_selected)} selected features to 'lasso_selected_features.csv'\")\n",
    "\n",
    "# Save model comparison\n",
    "comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "print(\"Saved model comparison to 'model_comparison.csv'\")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Working)",
   "language": "python",
   "name": "python3-working"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
